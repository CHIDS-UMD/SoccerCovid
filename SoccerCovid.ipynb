{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef7c69d-dccc-41ae-af85-5e1c997d617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e707dc-83fd-4e71-b3f1-8eeaab909728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_raw(url):\n",
    "    #TODO: error handling\n",
    "    return BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "def get_match_schedule(raw):\n",
    "    return raw.find_all(\"time\")[0].string\n",
    "\n",
    "def get_venue_name(raw):\n",
    "    return raw.find_all(\"span\", {\"class\": \"sdc-site-match-header__detail-venue\"})[0].string\n",
    "\n",
    "def get_match_title(raw):\n",
    "    return raw.title.string\n",
    "\n",
    "def get_team_names(title, title_regex):\n",
    "    title_group = title_regex.match(title)\n",
    "    return title_group.group('teamA'), title_group.group('teamB')\n",
    "\n",
    "def get_team_scores(title, title_regex):\n",
    "    title_group = title_regex.match(title)\n",
    "    return title_group.group('teamA_score'), title_group.group('teamB_score')\n",
    "\n",
    "def get_winner_details(record):\n",
    "    if(record['teamA_score'] > record['teamB_score']):\n",
    "        return record['teamA'], record['teamA_score']\n",
    "    else:\n",
    "        return record['teamB'], record['teamB_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2822735-6de0-4694-bade-23cdd9149bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.skysports.com/premier-league-results/'\n",
    "\n",
    "def get_match_details(year_range):\n",
    "\n",
    "    response = requests.get(base_url+year_range) #Fetch HTML Page\n",
    "    if(response.status_code == 200):\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\") #Parse HTML Page\n",
    "        urls = soup.find_all(\"a\", {\"class\": \"matches__item matches__link\"}, limit=None)\n",
    "\n",
    "        #TODO: check for pagination\n",
    "        match_details = []\n",
    "        for url in urls[:2]: # Testing purpose\n",
    "            match_details.append(url.attrs['href'])\n",
    "\n",
    "        match_details = pd.DataFrame(match_details, columns =['url']) \n",
    "\n",
    "        match_details['raw'] = match_details['url'].apply(get_match_raw)\n",
    "        match_details['schedule'] = match_details['raw'].apply(get_match_schedule)\n",
    "        match_details['venue'] = match_details['raw'].apply(get_venue_name)\n",
    "        match_details['raw_title'] = match_details['raw'].apply(get_match_title)\n",
    "\n",
    "        title_regex = re.compile(r'(?P<teamA>[a-zA-Z_ \\']*) (?P<teamA_score>\\d?\\d) - (?P<teamB_score>\\d?\\d) (?P<teamB>[a-zA-Z_ \\']*) -')\n",
    "        match_details[['teamA', 'teamB']] = pd.DataFrame(match_details['raw_title']\n",
    "                                                         .apply(lambda x: get_team_names(x, title_regex))\n",
    "                                                         .tolist(), index=match_details.index)\n",
    "        match_details[['teamA_score', 'teamB_score']] = pd.DataFrame(match_details['raw_title']\n",
    "                                                                     .apply(lambda x: get_team_scores(x, title_regex))\n",
    "                                                                     .tolist(), index=match_details.index)\n",
    "        match_details[['winner_Team','winner_score']] = pd.DataFrame(match_details\n",
    "                                                                     .apply(get_winner_details, axis=1)\n",
    "                                                                     .tolist(), index=match_details.index)  \n",
    "        return match_details\n",
    "    else:\n",
    "        print(f'{base_url} request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc67c726-e466-47cc-8cb1-c6e4b4820267",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = 'C:/webdrivers/chromedriver'\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# Link config\n",
    "google_base_url = 'https://www.google.com/search?q='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e233db6-63ae-4ba8-94e9-8e7c68a6a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoccerCovid:\n",
    "    def __init__(self):\n",
    "        self.final_data = {}\n",
    "        pass\n",
    "\n",
    "    def get_driver(self):\n",
    "        \"\"\"\n",
    "        Returns a new instance of selenium webdriver\n",
    "        \"\"\"\n",
    "        return webdriver.Chrome(service=service)\n",
    "    \n",
    "    def get_url(self, param):\n",
    "        param += ' wikipedia'\n",
    "        param = param.replace(\" \", \"+\") \n",
    "        return ''.join([google_base_url, param])\n",
    "    \n",
    "    def get_by_xpath(self, driver, xpath):\n",
    "        return driver.find_elements(By.XPATH, xpath)\n",
    "        \n",
    "    def navigate_to_site(self, driver, url):\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def get_wiki_url(self, driver, param, ext = False):\n",
    "        param += ' Football Club' if ext else ''\n",
    "        url = self.get_url(param)\n",
    "        self.navigate_to_site(driver, url)\n",
    "        xpath = \"//a[contains(@href, 'wikipedia.org/wiki')]\"\n",
    "        url_list = driver.find_elements(By.XPATH, xpath)\n",
    "        return url_list[0].get_attribute('href')\n",
    "    \n",
    "    def get_venue_location(self, match, driver):\n",
    "        wiki_url = self.get_wiki_url(driver, match['venue'])\n",
    "        \n",
    "        self.navigate_to_site(driver, wiki_url)  \n",
    "        xpath = \"//th[contains(text(), 'Location' )]/following-sibling::td\"\n",
    "        return self.get_by_xpath(driver, xpath)[0].text\n",
    "    \n",
    "    def get_winner_location(self, match, driver):\n",
    "        wiki_url = self.get_wiki_url(driver, match['winner_Team'], True)\n",
    "        self.navigate_to_site(driver, wiki_url)\n",
    "        \n",
    "        xpath = \"//th[contains(text(), 'Ground' )]/following-sibling::td/a\"\n",
    "        venue_url = self.get_by_xpath(driver, xpath)[0].get_attribute('href')\n",
    "        \n",
    "        self.navigate_to_site(driver, venue_url)\n",
    "        \n",
    "        xpath = \"//th[contains(text(), 'Location' )]/following-sibling::td\"\n",
    "        return self.get_by_xpath(driver, xpath)[0].text\n",
    "    \n",
    "    def scrape_pages(self, match_details):\n",
    "        driver = self.get_driver()\n",
    "        match_details['venue_location'] = match_details.apply(lambda x: self.get_venue_location(x, driver), axis = 1)\n",
    "        match_details['winner_location'] = match_details.apply(lambda x: self.get_winner_location(x, driver), axis = 1)\n",
    "        driver.quit()\n",
    "        return match_details\n",
    "    \n",
    "    def start_scraping(self, match_details_df):\n",
    "        return self.scrape_pages(match_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eede3231-6914-42e1-baab-be6b62707efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-20', '2020-21']\n",
    "\n",
    "for date in dates:\n",
    "    file_name = 'covid_soccer_' + date + '.csv'\n",
    "    if not os.path.exists(file_name):\n",
    "        match_details_df = get_match_details(date)\n",
    "        result_df = SoccerCovid().start_scraping(match_details_df)\n",
    "        result_df.drop(columns=['raw','raw_title'], inplace=True)\n",
    "        result_df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9ee9b-a711-4c1b-bc0c-ce7bb670925d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9998804b-68ef-4ee0-8928-803378525be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual Regex extraction\n",
    "# url = 'https://www.skysports.com/premier-league-results/2019-20'\n",
    "# site = str(requests.get(url).content)\n",
    "# regex = r'<a href=\"(https:\\/\\/www\\.skysports\\.com\\/football\\/[a-zA-Z-\\/]+\\d+)\" class=\"matches__item matches__link\"'\n",
    "# result = re.findall(regex, site)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
